{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ffbb9256",
      "metadata": {
        "papermill": {
          "duration": 0.006964,
          "end_time": "2025-01-31T11:59:37.074738",
          "exception": false,
          "start_time": "2025-01-31T11:59:37.067774",
          "status": "completed"
        },
        "tags": [],
        "id": "ffbb9256"
      },
      "source": [
        "# **《《《 YOLO 》》》**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60492643",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-01-31T11:59:37.084609Z",
          "iopub.status.busy": "2025-01-31T11:59:37.084168Z",
          "iopub.status.idle": "2025-01-31T12:01:17.347686Z",
          "shell.execute_reply": "2025-01-31T12:01:17.346790Z"
        },
        "papermill": {
          "duration": 100.274269,
          "end_time": "2025-01-31T12:01:17.353418",
          "exception": false,
          "start_time": "2025-01-31T11:59:37.079149",
          "status": "completed"
        },
        "tags": [],
        "id": "60492643"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!tar xfvz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n",
        "!pip install --no-index --find-links=./packages ultralytics\n",
        "!rm -rf ./packages\n",
        "try:\n",
        "    import zarr\n",
        "except:\n",
        "    !cp -r '/kaggle/input/hengck-czii-cryo-et-01/wheel_file' '/kaggle/working/'\n",
        "    !pip install /kaggle/working/wheel_file/asciitree-0.3.3/asciitree-0.3.3\n",
        "    !pip install --no-index --find-links=/kaggle/working/wheel_file zarr\n",
        "    !pip install --no-index --find-links=/kaggle/working/wheel_file connected-components-3d\n",
        "from typing import List, Tuple, Union\n",
        "deps_path = '/kaggle/input/czii-cryoet-dependencies'\n",
        "! pip install -q --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt\n",
        "import lightning.pytorch as pl\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import sys\n",
        "sys.path.append('/kaggle/input/hengck-czii-cryo-et-01')\n",
        "from czii_helper import *\n",
        "from dataset import *\n",
        "from model2 import *\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f05353b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:01:17.365842Z",
          "iopub.status.busy": "2025-01-31T12:01:17.365585Z",
          "iopub.status.idle": "2025-01-31T12:01:20.155281Z",
          "shell.execute_reply": "2025-01-31T12:01:20.154548Z"
        },
        "papermill": {
          "duration": 2.797389,
          "end_time": "2025-01-31T12:01:20.156748",
          "exception": false,
          "start_time": "2025-01-31T12:01:17.359359",
          "status": "completed"
        },
        "tags": [],
        "id": "9f05353b",
        "outputId": "d5f56bc7-c3bd-49fc-ac5a-c6389fe88be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import sys\n",
        "import warnings\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "import zarr\n",
        "from scipy.spatial import cKDTree\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d080b9e0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:01:20.164724Z",
          "iopub.status.busy": "2025-01-31T12:01:20.164508Z",
          "iopub.status.idle": "2025-01-31T12:01:21.893921Z",
          "shell.execute_reply": "2025-01-31T12:01:21.893183Z"
        },
        "papermill": {
          "duration": 1.735043,
          "end_time": "2025-01-31T12:01:21.895580",
          "exception": false,
          "start_time": "2025-01-31T12:01:20.160537",
          "status": "completed"
        },
        "tags": [],
        "id": "d080b9e0"
      },
      "outputs": [],
      "source": [
        "model_path = '/kaggle/input/czii-yolo-l-trained-with-synthetic-data/best_synthetic.pt'\n",
        "model = YOLO(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f908046",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:01:21.904595Z",
          "iopub.status.busy": "2025-01-31T12:01:21.904330Z",
          "iopub.status.idle": "2025-01-31T12:01:21.971831Z",
          "shell.execute_reply": "2025-01-31T12:01:21.971273Z"
        },
        "papermill": {
          "duration": 0.073146,
          "end_time": "2025-01-31T12:01:21.973061",
          "exception": false,
          "start_time": "2025-01-31T12:01:21.899915",
          "status": "completed"
        },
        "tags": [],
        "id": "3f908046"
      },
      "outputs": [],
      "source": [
        "runs_path = '/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/*'\n",
        "runs = sorted(glob.glob(runs_path))\n",
        "runs = [os.path.basename(run) for run in runs]\n",
        "sp = len(runs)//2\n",
        "runs1 = runs[:sp]\n",
        "runs1[:5]\n",
        "\n",
        "#add by @minfuka\n",
        "runs2 = runs[sp:]\n",
        "runs2[:5]\n",
        "\n",
        "#add by @minfuka\n",
        "assert torch.cuda.device_count() == 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67a8587",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:01:21.980444Z",
          "iopub.status.busy": "2025-01-31T12:01:21.980183Z",
          "iopub.status.idle": "2025-01-31T12:01:21.983959Z",
          "shell.execute_reply": "2025-01-31T12:01:21.983415Z"
        },
        "papermill": {
          "duration": 0.008653,
          "end_time": "2025-01-31T12:01:21.985128",
          "exception": false,
          "start_time": "2025-01-31T12:01:21.976475",
          "status": "completed"
        },
        "tags": [],
        "id": "b67a8587"
      },
      "outputs": [],
      "source": [
        "particle_names = [\n",
        "    'apo-ferritin',\n",
        "    'beta-amylase',\n",
        "    'beta-galactosidase',\n",
        "    'ribosome',\n",
        "    'thyroglobulin',\n",
        "    'virus-like-particle'\n",
        "]\n",
        "\n",
        "particle_to_index = {\n",
        "    'apo-ferritin': 0,\n",
        "    'beta-amylase': 1,\n",
        "    'beta-galactosidase': 2,\n",
        "    'ribosome': 3,\n",
        "    'thyroglobulin': 4,\n",
        "    'virus-like-particle': 5\n",
        "}\n",
        "\n",
        "index_to_particle = {index: name for name, index in particle_to_index.items()}\n",
        "\n",
        "particle_radius = {\n",
        "    'apo-ferritin': 70,\n",
        "    'beta-amylase': 75,\n",
        "    'beta-galactosidase': 95,\n",
        "    'ribosome': 150,\n",
        "    'thyroglobulin': 135,\n",
        "    'virus-like-particle': 145,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "596559ba",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:01:21.992472Z",
          "iopub.status.busy": "2025-01-31T12:01:21.992260Z",
          "iopub.status.idle": "2025-01-31T12:01:22.009007Z",
          "shell.execute_reply": "2025-01-31T12:01:22.008463Z"
        },
        "papermill": {
          "duration": 0.021638,
          "end_time": "2025-01-31T12:01:22.010048",
          "exception": false,
          "start_time": "2025-01-31T12:01:21.988410",
          "status": "completed"
        },
        "tags": [],
        "id": "596559ba"
      },
      "outputs": [],
      "source": [
        "# add by @sesasj\n",
        "class UnionFind:\n",
        "    def __init__(self, size):\n",
        "        self.parent = np.arange(size)\n",
        "        self.rank = np.zeros(size, dtype=int)\n",
        "\n",
        "    def find(self, u):\n",
        "        if self.parent[u] != u:\n",
        "            self.parent[u] = self.find(self.parent[u])\n",
        "        return self.parent[u]\n",
        "\n",
        "    def union(self, u, v):\n",
        "        u_root = self.find(u)\n",
        "        v_root = self.find(v)\n",
        "        if u_root == v_root:\n",
        "            return\n",
        "\n",
        "        if self.rank[u_root] < self.rank[v_root]:\n",
        "            self.parent[u_root] = v_root\n",
        "        else:\n",
        "            self.parent[v_root] = u_root\n",
        "            if self.rank[u_root] == self.rank[v_root]:\n",
        "                self.rank[u_root] += 1\n",
        "\n",
        "class PredictionAggregator:\n",
        "    def __init__(self, first_conf=0.2, conf_coef=0.75):\n",
        "        self.first_conf = first_conf\n",
        "        self.conf_coef = conf_coef\n",
        "        self.particle_confs = np.array([0.5, 0.0, 0.2, 0.5, 0.2, 0.5])\n",
        "\n",
        "    def convert_to_8bit(self, volume):\n",
        "        lower, upper = np.percentile(volume, (0.5, 99.5))\n",
        "        clipped = np.clip(volume, lower, upper)\n",
        "        scaled = ((clipped - lower) / (upper - lower + 1e-12) * 255).astype(np.uint8)\n",
        "        return scaled\n",
        "\n",
        "    def make_predictions(self, run_id, model, device_no):\n",
        "        volume_path = f'/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/{run_id}/VoxelSpacing10.000/denoised.zarr'\n",
        "        volume = zarr.open(volume_path, mode='r')[0]\n",
        "        volume_8bit = self.convert_to_8bit(volume)\n",
        "        num_slices = volume_8bit.shape[0]\n",
        "\n",
        "        detections = {\n",
        "            'particle_type': [],\n",
        "            'confidence': [],\n",
        "            'x': [],\n",
        "            'y': [],\n",
        "            'z': []\n",
        "        }\n",
        "\n",
        "        for slice_idx in range(num_slices):\n",
        "\n",
        "            img = volume_8bit[slice_idx]\n",
        "            input_image = cv2.resize(np.stack([img]*3, axis=-1), (640, 640))\n",
        "\n",
        "            results = model.predict(\n",
        "                input_image,\n",
        "                save=False,\n",
        "                imgsz=640,\n",
        "                conf=self.first_conf,\n",
        "                device=device_no,\n",
        "                batch=1,\n",
        "                verbose=False,\n",
        "            )\n",
        "\n",
        "            for result in results:\n",
        "                boxes = result.boxes\n",
        "                if boxes is None:\n",
        "                    continue\n",
        "                cls = boxes.cls.cpu().numpy().astype(int)\n",
        "                conf = boxes.conf.cpu().numpy()\n",
        "                xyxy = boxes.xyxy.cpu().numpy()\n",
        "\n",
        "                xc = ((xyxy[:, 0] + xyxy[:, 2]) / 2.0) * 10 * (63/64) # 63/64 because of the resize\n",
        "                yc = ((xyxy[:, 1] + xyxy[:, 3]) / 2.0) * 10 * (63/64)\n",
        "                zc = np.full(xc.shape, slice_idx * 10 + 5)\n",
        "\n",
        "                particle_types = [index_to_particle[c] for c in cls]\n",
        "\n",
        "                detections['particle_type'].extend(particle_types)\n",
        "                detections['confidence'].extend(conf)\n",
        "                detections['x'].extend(xc)\n",
        "                detections['y'].extend(yc)\n",
        "                detections['z'].extend(zc)\n",
        "\n",
        "        if not detections['particle_type']:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        particle_types = np.array(detections['particle_type'])\n",
        "        confidences = np.array(detections['confidence'])\n",
        "        xs = np.array(detections['x'])\n",
        "        ys = np.array(detections['y'])\n",
        "        zs = np.array(detections['z'])\n",
        "\n",
        "        aggregated_data = []\n",
        "\n",
        "        for idx, particle in enumerate(particle_names):\n",
        "            if particle == 'beta-amylase':\n",
        "                continue\n",
        "\n",
        "            mask = (particle_types == particle)\n",
        "            if not np.any(mask):\n",
        "                continue\n",
        "\n",
        "            particle_confidences = confidences[mask]\n",
        "            particle_xs = xs[mask]\n",
        "            particle_ys = ys[mask]\n",
        "            particle_zs = zs[mask]\n",
        "            # -------------modified by @sersasj ------------------------\n",
        "            coords = np.vstack((particle_xs, particle_ys, particle_zs)).T\n",
        "\n",
        "\n",
        "            z_distance = 20 # How many slices can you \"jump\" to aggregate predictions 10 = 1, 20 = 2...\n",
        "            xy_distance = 20 # xy_tol_p2 in original code by ITK8191\n",
        "\n",
        "            max_distance = math.sqrt(z_distance**2 + xy_distance**2)\n",
        "            tree = cKDTree(coords)\n",
        "            pairs = tree.query_pairs(r=max_distance, p=2)\n",
        "\n",
        "\n",
        "            uf = UnionFind(len(coords))\n",
        "\n",
        "            coords_xy = coords[:, :2]\n",
        "            coords_z = coords[:, 2]\n",
        "            for u, v in pairs:\n",
        "                z_diff = abs(coords_z[u] - coords_z[v])\n",
        "                if z_diff > z_distance:\n",
        "                    continue\n",
        "\n",
        "                xy_diff = np.linalg.norm(coords_xy[u] - coords_xy[v])\n",
        "                if xy_diff > xy_distance:\n",
        "                    continue\n",
        "\n",
        "                uf.union(u, v)\n",
        "\n",
        "            roots = np.array([uf.find(i) for i in range(len(coords))])\n",
        "            unique_roots, inverse_indices, counts = np.unique(roots, return_inverse=True, return_counts=True)\n",
        "            conf_sums = np.bincount(inverse_indices, weights=particle_confidences)\n",
        "\n",
        "            aggregated_confidences = conf_sums / (counts ** self.conf_coef)\n",
        "            cluster_per_particle = [4,1,2,9,4,8]\n",
        "            valid_clusters = (counts >= cluster_per_particle[idx]) & (aggregated_confidences > self.particle_confs[idx])\n",
        "\n",
        "            if not np.any(valid_clusters):\n",
        "                continue\n",
        "\n",
        "            cluster_ids = unique_roots[valid_clusters]\n",
        "\n",
        "            centers_x = np.bincount(inverse_indices, weights=particle_xs) / counts\n",
        "            centers_y = np.bincount(inverse_indices, weights=particle_ys) / counts\n",
        "            centers_z = np.bincount(inverse_indices, weights=particle_zs) / counts\n",
        "\n",
        "            centers_x = centers_x[valid_clusters]\n",
        "            centers_y = centers_y[valid_clusters]\n",
        "            centers_z = centers_z[valid_clusters]\n",
        "\n",
        "            aggregated_df = pd.DataFrame({\n",
        "                'experiment': [run_id] * len(centers_x),\n",
        "                'particle_type': [particle] * len(centers_x),\n",
        "                'x': centers_x,\n",
        "                'y': centers_y,\n",
        "                'z': centers_z\n",
        "            })\n",
        "\n",
        "            aggregated_data.append(aggregated_df)\n",
        "\n",
        "        if aggregated_data:\n",
        "            return pd.concat(aggregated_data, axis=0)\n",
        "        else:\n",
        "            return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ad2382",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:01:22.017236Z",
          "iopub.status.busy": "2025-01-31T12:01:22.017008Z",
          "iopub.status.idle": "2025-01-31T12:01:53.301766Z",
          "shell.execute_reply": "2025-01-31T12:01:53.300538Z"
        },
        "papermill": {
          "duration": 31.289738,
          "end_time": "2025-01-31T12:01:53.303106",
          "exception": false,
          "start_time": "2025-01-31T12:01:22.013368",
          "status": "completed"
        },
        "tags": [],
        "id": "f9ad2382",
        "outputId": "185897d0-b296-4e99-b7ea-da51987c0f1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:16<00:00, 16.12s/it]\n",
            "100%|██████████| 2/2 [00:30<00:00, 15.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "estimated total prediction time for 500 runs: 5212.5943 seconds\n"
          ]
        }
      ],
      "source": [
        "# instance main class\n",
        "aggregator = PredictionAggregator(first_conf=0.22,  conf_coef=0.39) #Update\n",
        "aggregated_results = []\n",
        "#add by @minfuka\n",
        "from concurrent.futures import ProcessPoolExecutor #add by @minfuka\n",
        "\n",
        "#add by @minfuka\n",
        "def inference(runs, model, device_no):\n",
        "    subs = []\n",
        "    for r in tqdm(runs, total=len(runs)):\n",
        "        df = aggregator.make_predictions(r, model, device_no)\n",
        "        subs.append(df)\n",
        "\n",
        "    return subs\n",
        "start_time = time.time()\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=2) as executor:\n",
        "    results = list(executor.map(inference, (runs1, runs2), (model, model), (\"0\", \"1\")))\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "estimated_total_time = (end_time - start_time) / len(runs) * 500\n",
        "print(f'estimated total prediction time for 500 runs: {estimated_total_time:.4f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114927b7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:01:53.311822Z",
          "iopub.status.busy": "2025-01-31T12:01:53.311566Z",
          "iopub.status.idle": "2025-01-31T12:01:53.317608Z",
          "shell.execute_reply": "2025-01-31T12:01:53.316973Z"
        },
        "papermill": {
          "duration": 0.01169,
          "end_time": "2025-01-31T12:01:53.318806",
          "exception": false,
          "start_time": "2025-01-31T12:01:53.307116",
          "status": "completed"
        },
        "tags": [],
        "id": "114927b7"
      },
      "outputs": [],
      "source": [
        "#change by @minfuka\n",
        "submission0 = pd.concat(results[0])\n",
        "submission1 = pd.concat(results[1])\n",
        "submission_ = pd.concat([submission0, submission1]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b43b557",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:01:53.327073Z",
          "iopub.status.busy": "2025-01-31T12:01:53.326859Z",
          "iopub.status.idle": "2025-01-31T12:01:53.336304Z",
          "shell.execute_reply": "2025-01-31T12:01:53.335687Z"
        },
        "papermill": {
          "duration": 0.014927,
          "end_time": "2025-01-31T12:01:53.337483",
          "exception": false,
          "start_time": "2025-01-31T12:01:53.322556",
          "status": "completed"
        },
        "tags": [],
        "id": "3b43b557"
      },
      "outputs": [],
      "source": [
        "submission_.insert(0, 'id', range(len(submission_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbe119b0",
      "metadata": {
        "papermill": {
          "duration": 0.003416,
          "end_time": "2025-01-31T12:01:53.344619",
          "exception": false,
          "start_time": "2025-01-31T12:01:53.341203",
          "status": "completed"
        },
        "tags": [],
        "id": "fbe119b0"
      },
      "source": [
        "# **《《《 Unet3D(Monai) 》》》**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c8631f7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:01:53.352728Z",
          "iopub.status.busy": "2025-01-31T12:01:53.352525Z",
          "iopub.status.idle": "2025-01-31T12:03:12.633622Z",
          "shell.execute_reply": "2025-01-31T12:03:12.632851Z"
        },
        "papermill": {
          "duration": 79.286953,
          "end_time": "2025-01-31T12:03:12.635136",
          "exception": false,
          "start_time": "2025-01-31T12:01:53.348183",
          "status": "completed"
        },
        "tags": [],
        "id": "2c8631f7",
        "outputId": "a2ed83b9-2c5b-4802-bb58-55feda852df9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-07ea58750c60>:155: DeprecationWarning: config_type not found in config file, defaulting to filesystem\n",
            "  root = copick.from_file(copick_test_config_path)\n",
            "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 135.36it/s]\n",
            "100%|██████████| 98/98 [00:12<00:00,  8.15it/s]\n",
            "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 183.02it/s]\n",
            "100%|██████████| 98/98 [00:11<00:00,  8.51it/s]\n",
            "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 147.25it/s]\n",
            "100%|██████████| 98/98 [00:11<00:00,  8.35it/s]\n"
          ]
        }
      ],
      "source": [
        "class Model(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int = 3,\n",
        "        in_channels: int = 1,\n",
        "        out_channels: int = 7,\n",
        "        channels: Union[Tuple[int, ...], List[int]] = (48, 64, 80, 80),\n",
        "        strides: Union[Tuple[int, ...], List[int]] = (2, 2, 1),\n",
        "        num_res_units: int = 1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = UNet(\n",
        "            spatial_dims=self.hparams.spatial_dims,\n",
        "            in_channels=self.hparams.in_channels,\n",
        "            out_channels=self.hparams.out_channels,\n",
        "            channels=self.hparams.channels,\n",
        "            strides=self.hparams.strides,\n",
        "            num_res_units=self.hparams.num_res_units,\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "channels = (48, 64, 80, 80)\n",
        "strides_pattern = (2, 2, 1)\n",
        "num_res_units = 1\n",
        "def extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
        "    if not arrays or not isinstance(arrays, list):\n",
        "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
        "\n",
        "    # Verify all arrays have the same shape\n",
        "    shape = arrays[0].shape\n",
        "    if not all(arr.shape == shape for arr in arrays):\n",
        "        raise ValueError(\"All input arrays must have the same shape\")\n",
        "\n",
        "    if patch_size > min(shape):\n",
        "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n",
        "\n",
        "    m, n, l = shape\n",
        "    patches = []\n",
        "    coordinates = []\n",
        "\n",
        "    # Calculate starting positions for each dimension\n",
        "    x_starts = calculate_patch_starts(m, patch_size)\n",
        "    y_starts = calculate_patch_starts(n, patch_size)\n",
        "    z_starts = calculate_patch_starts(l, patch_size)\n",
        "\n",
        "    # Extract patches from each array\n",
        "    for arr in arrays:\n",
        "        for x in x_starts:\n",
        "            for y in y_starts:\n",
        "                for z in z_starts:\n",
        "                    patch = arr[\n",
        "                        x:x + patch_size,\n",
        "                        y:y + patch_size,\n",
        "                        z:z + patch_size\n",
        "                    ]\n",
        "                    patches.append(patch)\n",
        "                    coordinates.append((x, y, z))\n",
        "\n",
        "    return patches, coordinates\n",
        "def reconstruct_array(patches: List[np.ndarray],\n",
        "                     coordinates: List[Tuple[int, int, int]],\n",
        "                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n",
        "    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n",
        "\n",
        "    patch_size = patches[0].shape[0]\n",
        "\n",
        "    for patch, (x, y, z) in zip(patches, coordinates):\n",
        "        reconstructed[\n",
        "            x:x + patch_size,\n",
        "            y:y + patch_size,\n",
        "            z:z + patch_size\n",
        "        ] = patch\n",
        "\n",
        "\n",
        "    return reconstructed\n",
        "def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n",
        "    if dimension_size <= patch_size:\n",
        "        return [0]\n",
        "\n",
        "    # Calculate number of patches needed\n",
        "    n_patches = np.ceil(dimension_size / patch_size)\n",
        "\n",
        "    if n_patches == 1:\n",
        "        return [0]\n",
        "\n",
        "    # Calculate overlap\n",
        "    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n",
        "\n",
        "    # Generate starting positions\n",
        "    positions = []\n",
        "    for i in range(int(n_patches)):\n",
        "        pos = int(i * (patch_size - total_overlap))\n",
        "        if pos + patch_size > dimension_size:\n",
        "            pos = dimension_size - patch_size\n",
        "        if pos not in positions:  # Avoid duplicates\n",
        "            positions.append(pos)\n",
        "\n",
        "    return positions\n",
        "import pandas as pd\n",
        "\n",
        "def dict_to_df(coord_dict, experiment_name):\n",
        "    # Create lists to store data\n",
        "    all_coords = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Process each label and its coordinates\n",
        "    for label, coords in coord_dict.items():\n",
        "        all_coords.append(coords)\n",
        "        all_labels.extend([label] * len(coords))\n",
        "\n",
        "    # Concatenate all coordinates\n",
        "    all_coords = np.vstack(all_coords)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'experiment': experiment_name,\n",
        "        'particle_type': all_labels,\n",
        "        'x': all_coords[:, 0],\n",
        "        'y': all_coords[:, 1],\n",
        "        'z': all_coords[:, 2]\n",
        "    })\n",
        "\n",
        "\n",
        "    return df\n",
        "from typing import List, Tuple, Union\n",
        "import numpy as np\n",
        "import torch\n",
        "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    EnsureChannelFirstd,\n",
        "    Orientationd,\n",
        "    AsDiscrete,\n",
        "    RandFlipd,\n",
        "    RandRotate90d,\n",
        "    NormalizeIntensityd,\n",
        "    RandCropByLabelClassesd,\n",
        ")\n",
        "TRAIN_DATA_DIR = \"/kaggle/input/create-numpy-dataset-exp-name\"\n",
        "import json\n",
        "copick_config_path = TRAIN_DATA_DIR + \"/copick.config\"\n",
        "\n",
        "with open(copick_config_path) as f:\n",
        "    copick_config = json.load(f)\n",
        "\n",
        "copick_config['static_root'] = '/kaggle/input/czii-cryo-et-object-identification/test/static'\n",
        "\n",
        "copick_test_config_path = 'copick_test.config'\n",
        "\n",
        "with open(copick_test_config_path, 'w') as outfile:\n",
        "    json.dump(copick_config, outfile)\n",
        "import copick\n",
        "\n",
        "root = copick.from_file(copick_test_config_path)\n",
        "\n",
        "copick_user_name = \"copickUtils\"\n",
        "copick_segmentation_name = \"paintedPicks\"\n",
        "voxel_size = 11\n",
        "tomo_type = \"denoised\"\n",
        "inference_transforms = Compose([\n",
        "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
        "    NormalizeIntensityd(keys=\"image\"),\n",
        "    Orientationd(keys=[\"image\"], axcodes=\"RAS\")\n",
        "])\n",
        "import cc3d\n",
        "\n",
        "id_to_name = {1: \"apo-ferritin\",\n",
        "              2: \"beta-amylase\",\n",
        "              3: \"beta-galactosidase\",\n",
        "              4: \"ribosome\",\n",
        "              5: \"thyroglobulin\",\n",
        "              6: \"virus-like-particle\"}\n",
        "BLOB_THRESHOLD = 255\n",
        "CERTAINTY_THRESHOLD = 0.05\n",
        "\n",
        "classes = [1, 2, 3, 4, 5, 6]\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cc3d\n",
        "from monai.data import CacheDataset\n",
        "from monai.transforms import Compose, EnsureType\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from monai.networks.nets import UNet\n",
        "from monai.losses import TverskyLoss\n",
        "from monai.metrics import DiceMetric\n",
        "\n",
        "def load_models(model_paths):\n",
        "    models = []\n",
        "    for model_path in model_paths:\n",
        "        channels = (48, 64, 80, 80)\n",
        "        strides_pattern = (2, 2, 1)\n",
        "        num_res_units = 1\n",
        "        learning_rate = 1e-3\n",
        "        num_epochs = 100\n",
        "        model = Model(channels=channels, strides=strides_pattern, num_res_units=num_res_units)\n",
        "\n",
        "        weights =torch.load(model_path)['state_dict']\n",
        "        model.load_state_dict(weights)\n",
        "        model.to('cuda')\n",
        "        model.eval()\n",
        "        models.append(model)\n",
        "    return models\n",
        "\n",
        "\n",
        "model_paths = [\n",
        "    '/kaggle/input/cziials-a-230-unet/UNet-Model-val_metric0.450.ckpt',\n",
        "]\n",
        "\n",
        "\n",
        "models = load_models(model_paths)\n",
        "def ensemble_prediction_tta(models, input_tensor, threshold=0.5):\n",
        "    probs_list = []\n",
        "    data_copy0 = input_tensor.clone()\n",
        "    data_copy0=torch.flip(data_copy0, dims=[2])\n",
        "    data_copy1 = input_tensor.clone()\n",
        "    data_copy1=torch.flip(data_copy1, dims=[3])\n",
        "    data_copy2 = input_tensor.clone()\n",
        "    data_copy2=torch.flip(data_copy2, dims=[4])\n",
        "    data_copy3 = input_tensor.clone()\n",
        "    data_copy3 = data_copy3.rot90(1, dims=[3, 4])\n",
        "    with torch.no_grad():\n",
        "        model_output0 = model(input_tensor)\n",
        "        model_output1 = model(data_copy0)\n",
        "        model_output1=torch.flip(model_output1, dims=[2])\n",
        "        model_output2 = model(data_copy1)\n",
        "        model_output2=torch.flip(model_output2, dims=[3])\n",
        "        model_output3 = model(data_copy2)\n",
        "        model_output3=torch.flip(model_output3, dims=[4])\n",
        "        probs0 = torch.softmax(model_output0[0], dim=0)\n",
        "        probs1 = torch.softmax(model_output1[0], dim=0)\n",
        "        probs2 = torch.softmax(model_output2[0], dim=0)\n",
        "        probs3 = torch.softmax(model_output3[0], dim=0)\n",
        "        probs_list.append(probs0)\n",
        "        probs_list.append(probs1)\n",
        "        probs_list.append(probs2)\n",
        "        probs_list.append(probs3)\n",
        "    avg_probs = torch.mean(torch.stack(probs_list), dim=0)\n",
        "    thresh_probs = avg_probs > threshold\n",
        "    _, max_classes = thresh_probs.max(dim=0)\n",
        "    return max_classes\n",
        "sub=[]\n",
        "for model in models:\n",
        "    with torch.no_grad():\n",
        "        location_df = []\n",
        "        for run in root.runs:\n",
        "            tomo = run.get_voxel_spacing(10)\n",
        "            tomo = tomo.get_tomogram(tomo_type).numpy()\n",
        "            tomo_patches, coordinates = extract_3d_patches_minimal_overlap([tomo], 96)\n",
        "            tomo_patched_data = [{\"image\": img} for img in tomo_patches]\n",
        "            tomo_ds = CacheDataset(data=tomo_patched_data, transform=inference_transforms, cache_rate=1.0)\n",
        "            pred_masks = []\n",
        "            for i in tqdm(range(len(tomo_ds))):\n",
        "                input_tensor = tomo_ds[i]['image'].unsqueeze(0).to(\"cuda\")\n",
        "                max_classes = ensemble_prediction_tta(models, input_tensor, threshold=CERTAINTY_THRESHOLD)\n",
        "                pred_masks.append(max_classes.cpu().numpy())\n",
        "            reconstructed_mask = reconstruct_array(pred_masks, coordinates, tomo.shape)\n",
        "            location = {}\n",
        "            for c in classes:\n",
        "                cc = cc3d.connected_components(reconstructed_mask == c)\n",
        "                stats = cc3d.statistics(cc)\n",
        "                zyx = stats['centroids'][1:] * 10.012444  # 转换单位\n",
        "                zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]\n",
        "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])\n",
        "                location[id_to_name[c]] = xyz\n",
        "            df = dict_to_df(location, run.name)\n",
        "            location_df.append(df)\n",
        "        location_df = pd.concat(location_df)\n",
        "        location_df.insert(loc=0, column='id', value=np.arange(len(location_df)))\n",
        "        #최종결과물은 location_df라는 데이터프레임."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b047d6d",
      "metadata": {
        "papermill": {
          "duration": 0.017263,
          "end_time": "2025-01-31T12:03:12.671037",
          "exception": false,
          "start_time": "2025-01-31T12:03:12.653774",
          "status": "completed"
        },
        "tags": [],
        "id": "7b047d6d"
      },
      "source": [
        "# **《《《 Finaly Blend 》》》**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "441fb406",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-31T12:03:12.707104Z",
          "iopub.status.busy": "2025-01-31T12:03:12.706408Z",
          "iopub.status.idle": "2025-01-31T12:03:14.878831Z",
          "shell.execute_reply": "2025-01-31T12:03:14.877850Z"
        },
        "papermill": {
          "duration": 2.191948,
          "end_time": "2025-01-31T12:03:14.880497",
          "exception": false,
          "start_time": "2025-01-31T12:03:12.688549",
          "status": "completed"
        },
        "tags": [],
        "id": "441fb406"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "df = pd.concat([submission_,location_df], ignore_index=True)\n",
        "\n",
        "particle_names = ['apo-ferritin', 'beta-amylase', 'beta-galactosidase', 'ribosome', 'thyroglobulin', 'virus-like-particle']\n",
        "particle_radius = {\n",
        "    'apo-ferritin': 65,\n",
        "    'beta-amylase': 65,\n",
        "    'beta-galactosidase': 95,\n",
        "    'ribosome': 150,\n",
        "    'thyroglobulin': 135,\n",
        "    'virus-like-particle': 145,\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "final = []\n",
        "for pidx, p in enumerate(particle_names):\n",
        "    pdf = df[df['particle_type'] == p].reset_index(drop=True)\n",
        "    p_rad = particle_radius[p] ################################\n",
        "\n",
        "    grouped = pdf.groupby(['experiment'])\n",
        "\n",
        "    for exp, group in grouped:\n",
        "        group = group.reset_index(drop=True)\n",
        "\n",
        "        coords = group[['x', 'y', 'z']].values\n",
        "        db = DBSCAN(eps=p_rad, min_samples=2, metric='euclidean').fit(coords) ###############################3\n",
        "        labels = db.labels_\n",
        "\n",
        "        group['cluster'] = labels\n",
        "\n",
        "        for cluster_id in np.unique(labels):\n",
        "            if cluster_id == -1:\n",
        "                continue\n",
        "\n",
        "            cluster_points = group[group['cluster'] == cluster_id]\n",
        "\n",
        "            avg_x = cluster_points['x'].mean()\n",
        "            avg_y = cluster_points['y'].mean()\n",
        "            avg_z = cluster_points['z'].mean()\n",
        "\n",
        "            group.loc[group['cluster'] == cluster_id, ['x', 'y', 'z']] = avg_x, avg_y, avg_z\n",
        "            group = group.drop_duplicates(subset=['x', 'y', 'z'])\n",
        "        final.append(group)\n",
        "\n",
        "df_save = pd.concat(final, ignore_index=True)\n",
        "df_save = df_save.drop(columns=['cluster'])\n",
        "df_save = df_save.sort_values(by=['experiment', 'particle_type']).reset_index(drop=True)\n",
        "df_save['id'] = np.arange(0, len(df_save))\n",
        "df_save.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af62ace3",
      "metadata": {
        "papermill": {
          "duration": 0.017164,
          "end_time": "2025-01-31T12:03:14.916187",
          "exception": false,
          "start_time": "2025-01-31T12:03:14.899023",
          "status": "completed"
        },
        "tags": [],
        "id": "af62ace3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 10033515,
          "sourceId": 84969,
          "sourceType": "competition"
        },
        {
          "datasetId": 6052780,
          "sourceId": 9862305,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6040935,
          "sourceId": 9867543,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6465904,
          "sourceId": 10445850,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6484063,
          "sourceId": 10471985,
          "sourceType": "datasetVersion"
        },
        {
          "sourceId": 206640467,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 211097053,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 224.281461,
      "end_time": "2025-01-31T12:03:18.279120",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-01-31T11:59:33.997659",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}